% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoCompBoostBase.R
\name{AutoCompBoostBase}
\alias{AutoCompBoostBase}
\title{AutoCompBoostBase}
\arguments{
\item{task}{(\code{\link[mlr3:Task]{Task}}) \cr
Contains the task to be solved. Currently \code{\link[mlr3:TaskClassif]{TaskClassif}} and \code{\link[mlr3:TaskRegr]{TaskRegr}} are supported.}

\item{resampling}{(\link[mlr3:Resampling]{Resampling}) \cr
Contains the resampling method to be used for hyper-parameter optimization.
Defaults to \link[mlr3:mlr_resamplings_cv]{ResamplingCV} with 3 folds.}

\item{measure}{(\link[mlr3:Measure]{Measure}) \cr
Contains the performance measure, for which we optimize during training. \cr
Defaults to \link[mlr3measures:acc]{Accuracy} for classification and \link[mlr3measures:rmse]{RMSE} for regression.}

\item{tuning_method}{(\code{character(1)}) \cr
Tuning method. Possible choices are \code{"mbo"}, \code{"hyperband"} or \code{"smash"}¸ Default is \code{"mbo"}.}

\item{tuning_time}{(\code{integer(1)}) \cr
Termination criterium. Number of seconds for which to run the optimization. Does \emph{not} include training time of the final model. \cr
Default is set to \code{3600}, i.e. one hour. Tuning is terminated depending on the first termination criteria fulfilled.}

\item{tuning_iters}{(\code{integer(1)}) \cr
Termination criterium. Number of MBO iterations for which to run the optimization. \cr
Default is set to \code{150} iterations. Tuning is terminated depending on the first termination criteria fulfilled.}

\item{final_model}{(\code{logical(1)}) \cr
Whether or not to return the final model trained on the whole dataset at the end.}

\item{data}{(\link{data.frame} | \link{data.table} | \link[mlr3:Task]{Task}) \cr
New observations to be predicted. If \code{NULL}, defaults to the task the model
was trained on.}

\item{row_ids}{(\code{integer()}) \cr
Vector of training indices.}

\item{spline}{(\code{character(1L)}) \cr
Name of spline to plot.}
}
\value{
\link[=AutoCompBoostBase]{AutoCompBoostBase}

\code{\link[mlr3:PredictionClassif]{PredictionClassif}} | \code{\link[mlr3:PredictionRegr]{PredictionRegr}}

\code{\link[mlr3:ResampleResult]{ResampleResult}}

\code{\link[data.table:data.table]{data.table}}

\code{\link[data.table:data.table]{data.table}}

\code{\link[compboost:Compboost]{Compboost}}

(\code{character()})

\code{\link[ggplot2:ggplot]{ggplot}}

\code{\link[ggplot2:ggplot]{ggplot}}

\code{\link[ggplot2:ggplot]{ggplot}}
}
\description{
Base class for AutoCompBoost. Has subclasses for Classification and Regression.

Creates a new instance of this \link[R6:R6Class]{R6} class.

Trains the AutoML system.

Returns a \link[mlr3:Prediction]{Prediction} object for the given data based on the trained model.

Performs nested resampling. \code{\link[mlr3:mlr_resamplings_holdout]{ResamplingHoldout}} is used for the outer resampling.

Helper to extract the best hyperparameters from a tuned model.

Returns the model summary

Returns the trained model if \code{final_model} is set to TRUE.

Returns the selected base learners by the final model.

Plot function to plot a single spline.

Plot function to plot the feature importance.

Plot function to plot the learner traces.
}
\section{Fields}{

\describe{
\item{\code{task}}{(\code{\link[mlr3:Task]{Task}}) \cr
Contains the task to be solved.}

\item{\code{learner}}{(\link[mlr3tuning:AutoTuner]{AutoTuner}) \cr
The ML pipeline at the core of mlr3automl is an \link[mlr3tuning:AutoTuner]{AutoTuner} containing a \link[mlr3pipelines:mlr_learners_graph]{GraphLearner}.}

\item{\code{resampling}}{(\link[mlr3:Resampling]{Resampling}) \cr
Contains the resampling method to be used for hyper-parameter optimization.}

\item{\code{measure}}{(\link[mlr3:Measure]{Measure}) \cr
Contains the performance measure, for which we optimize during training. \cr}

\item{\code{tuning_method}}{(\code{character(1)}) \cr
Tuning method. Possible choices are \code{"mbo"}, \code{"hyperband"} or \code{"smash"}¸ Default is \code{"mbo"}.}

\item{\code{tuning_time}}{(\code{integer(1)}) \cr
Termination criterium. Number of seconds for which to run the optimization. Does \emph{not} include training time of the final model. \cr
Default is set to \code{60}, i.e. one minuet. Tuning is terminated depending on the first termination criteria fulfilled.}

\item{\code{tuning_iters}}{(\code{integer(1)}) \cr
Termination criterium. Number of MBO iterations for which to run the optimization. \cr
Default is set to \code{150} iterations. Tuning is terminated depending on the first termination criteria fulfilled.}

\item{\code{final_model}}{(\code{logical(1)}) \cr
Whether or not to return the final model trained on the whole dataset at the end.}

\item{\code{tuner}}{(\link[mlrintermbo:OptimizerInterMBO]{TunerInterMBO}) \cr
Tuning is performed using \link[mlrintermbo:OptimizerInterMBO]{TunerInterMBO}.}

\item{\code{tuning_terminator}}{(\link[bbotk:Terminator]{Terminator}) \cr
Contains an termination criterion for model tuning. \cr}
}}

\section{Internals}{

The AutoCompBoostBase class uses \link{mlr3pipelines} to create a machine learning pipeline. \cr
This pipeline contains multiple preprocessing steps wrapped in a \link[mlr3pipelines:mlr_learners_graph]{GraphLearner}. \cr
This \link[mlr3pipelines:mlr_learners_graph]{GraphLearner} is wrapped in an \link[mlr3tuning:AutoTuner]{AutoTuner} for Hyperparameter Optimization and proper resampling. \cr
Tuning is performed using Bayesian Optimization.
}

\section{Construction}{

Objects should be created using the \link[=AutoCompBoost]{AutoCompBoost} interface function.\preformatted{model = AutoCompBoost(task, resampling, measure, tuning_budget, tuning_iters, final_model)
}
}

